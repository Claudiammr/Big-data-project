{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"App\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a40d36499bbcc0a6",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load CSV File with Header"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d1f02403b8d20b1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load the CSV file with the first row as a header\n",
    "df = spark.read.format(\"csv\").option(\"delimiter\", \",\").option(\"header\", \"true\").load(\"1987.csv\")\n",
    "\n",
    "# Display the columns and the first 15 rows\n",
    "df.show(15, truncate=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29ddc359a0ca93d5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0a44fd-92a8-4c32-9acd-ba2a71946d10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Iterate over all columns in the DataFrame\n",
    "for column in df.columns:\n",
    "    df = df.withColumn(column, when(col(column) == \"NA\", None).otherwise(col(column)))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baba2d3d-2306-44ab-a74d-095f157c2e07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Counts the number of null values for each column\n",
    "df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ee0a15-f29b-461b-a029-07104055fa09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mostrar las columnas del DataFrame\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a860ae-2232-471a-9cdf-db8f21a51dc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0e748d-08b5-4a81-bd26-5eb6b287c520",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "- eliminate unnecesary variables\n",
    "- missing and duplicates values\n",
    "- see correlation\n",
    "- variable transformation\n",
    "- variable creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19529b89-a015-4f2a-a06f-1b699d5b9cb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# columns to eliminate\n",
    "columns = [\n",
    "    \"ArrTime\", \n",
    "    \"ActualElapsedTime\", \n",
    "    \"AirTime\", \n",
    "    \"TaxiIn\", \n",
    "    \"Diverted\", \n",
    "    \"CarrierDelay\", \n",
    "    \"WeatherDelay\", \n",
    "    \"NASDelay\", \n",
    "    \"SecurityDelay\", \n",
    "    \"LateAircraftDelay\"\n",
    "]\n",
    "\n",
    "# Eliminate columns\n",
    "df = df.drop(*columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35883cdd-2edc-47ba-a780-e30efd94a0ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07a7991-2e7e-4bbc-8b42-060f43af487c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# columns to eliminate\n",
    "columns = [\n",
    "    \"Year\",\n",
    "    \"TailNum\",\n",
    "    \"TaxiOut\",\n",
    "    \"Cancelled\",\n",
    "    \"CancellationCode\"  \n",
    "]\n",
    "\n",
    "# Eliminate columns\n",
    "df = df.drop(*columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c332197c-6b29-42bd-9eb4-e791713a1d16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6081fe72-7891-41b4-8766-e90e8691304d",
   "metadata": {},
   "source": [
    "## Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4648b34a-0027-4839-9895-917333067aef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Contar valores NA por columna\n",
    "df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "84b193de858d8c34",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Now we check if NA stands for 0. If this value is not present, means that NA was 0."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a66b2cce1bc21e91"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87379505-3215-428c-8bbe-7b734213ae6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "# Filter the DataFrame to keep only rows where ArrDelay is equal to 0\n",
    "filtered_df = df.filter(F.col(\"ArrDelay\") == 0)\n",
    "\n",
    "# Total number of rows in the DataFrame\n",
    "total_rows = df.count()\n",
    "\n",
    "# Check if there are any rows in the filtered DataFrame\n",
    "if filtered_df.count() > 0:\n",
    "    print(\"0 is present in the ArrDelay column \" + str(filtered_df.count()) + \" times out of \" + str(total_rows) + \".\")\n",
    "else:\n",
    "    print(\"0 is not present in the ArrDelay column.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0df10f9-d1d4-4d2a-b254-828a9caac563",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the percentage of null values for each column\n",
    "null_percentage = df.select([(count(when(col(c).isNull(), c)) / total_rows).alias(c) for c in df.columns])\n",
    "\n",
    "# Show the percentage of null values for each column\n",
    "null_percentage.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5ff12d-d8ee-40b4-aa65-441f1e07c540",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop rows with at least one missing value\n",
    "\n",
    "df = df.dropna()\n",
    "dropped_rows = total_rows - df.count()\n",
    "print(\"Dropped \"+ str(dropped_rows)+ \" rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee9a02d-dd7e-43fd-9e56-7c38ef749809",
   "metadata": {},
   "source": [
    "## Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.printSchema()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d7c298930db6b6a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53d608a-f26a-4550-b9f9-67f0dfc32658",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check for duplicates and show the results\n",
    "total_rows = df.count()\n",
    "df = df.dropDuplicates()\n",
    "\n",
    "if total_rows - df.count()  > 0:\n",
    "    print(\"There are duplicates in the DataFrame.\")\n",
    "else:\n",
    "    print(\"No duplicates found in the DataFrame.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882aec0f-ac1f-4104-bb46-2911e52015de",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97708e8b-3193-4499-8ee3-5da8f5c535bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d694f480-0a4f-4bd8-888e-1571a67c45a0",
   "metadata": {},
   "source": [
    "## Variable transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096b53ac-d091-49c6-b989-07346ac53b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to exclude from conversion\n",
    "exclude_columns = ['UniqueCarrier', 'Origin', 'Dest']\n",
    "\n",
    "# Convert all columns to integer type except the ones in exclude_columns\n",
    "for column in df.columns:\n",
    "    if column not in exclude_columns:\n",
    "        df = df.withColumn(column, col(column).cast(\"integer\"))\n",
    "\n",
    "# Display the columns and the first 15 rows to verify the change\n",
    "df.show(15, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Print distinct values for each specified column\n",
    "for column in exclude_columns:\n",
    "    print(f\"Distinct values in column '{column}':\")\n",
    "    distinct_values = df.select(column).distinct().collect()\n",
    "    for value in distinct_values:\n",
    "        print(value[column])\n",
    "    print(\"\\n\")  # Adding a newline for better readability\n",
    "    # Print the number of elements in the distinct_values list\n",
    "    print(f\"Number of distinct values: {len(distinct_values)}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14deb4462c6a2749",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import geographic coordinates"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d883a4781dc30abe"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "usa_airport_df = spark.read.csv(\"us-airports.csv\", header=True)\n",
    "\n",
    "# Select only the 'latitude_deg', 'longitude_deg', and 'iata_code' columns\n",
    "usa_airport_df = usa_airport_df.select(\"latitude_deg\", \"longitude_deg\", \"iata_code\")\n",
    "\n",
    "usa_airport_df.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14e90499fbadf24a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Select and concatenate the values from 'Origin' and 'Destination' columns\n",
    "origin = df.select(\"Origin\").distinct()\n",
    "destination = df.select(\"Dest\").distinct()\n",
    "\n",
    "all_airports = origin.distinct().union(destination)\n",
    "\n",
    "# Get distinct values\n",
    "distinct_airports = all_airports.distinct()\n",
    "print(all_airports.count())\n",
    "\n",
    "\n",
    "\n",
    "# Collect the distinct values\n",
    "\n",
    "#Distinct airport codes in the new Dataset.\n",
    "usa_iata_codes = usa_airport_df.select(\"iata_code\").distinct()\n",
    "\n",
    "print(\"Number of elements in the list:\", len(usa_airports_list))\n",
    "\n",
    "#Show the missing values\n",
    "\n",
    "distinct_airports.subtract(usa_iata_codes).show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8fc69cee8f70ed35",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to convert DMS (degrees, minutes, seconds) to DD (decimal degrees)\n",
    "def dms_to_dd(dms):\n",
    "    parts = re.split('[°′″]', dms)\n",
    "    degrees = float(parts[0])\n",
    "    minutes = float(parts[1])\n",
    "    seconds = float(parts[2])\n",
    "    direction = parts[3]\n",
    "    \n",
    "    dd = degrees + minutes/60 + seconds/3600\n",
    "    if direction in ('S', 'W'):\n",
    "        dd *= -1\n",
    "    return dd\n",
    "\n",
    "# Coordinates in DMS format with corresponding  codes\n",
    "coordinates = [\n",
    "    (\"43°08′36″N\", \"075°22′48″W\",\"UCA\"),\n",
    "    (\"18°20′14″N\", \"064°58′24″W\",\"STT\"),\n",
    "    (\"7°22′02″N\", \"134°32′39″E\",\"ROR\"),\n",
    "    (\"17°42′16″N\", \"64°48′06″W\",\"STX\"),\n",
    "    (\"9°29′56″N\", \"138°04′57″E\",\"YAP\"),\n",
    "    (\"13°29′02″N\", \"144°47′50″W\",\"GUM\"),\n",
    "    (\"15°07′08″N\", \"145°43′46″E\",\"SPN\"),\n",
    "    (\"30°12′44″N\", \"085°40′58″W\",\"PFN\"),\n",
    "    (\"18°26′22″N\", \"66°00′07″W\",\"SJU\")\n",
    "]\n",
    "\n",
    "# Convert the DMS coordinates to decimal degrees\n",
    "airport_coordinates_dd = [(dms_to_dd(lat), dms_to_dd(lon),code) for  lat, lon ,code in coordinates]\n",
    "\n",
    "new_airports_df = spark.createDataFrame(airport_coordinates_dd, [\"latitude_deg\", \"longitude_deg\", \"iata_code\"])\n",
    "new_airports_df.select(\"iata_code\").show()\n",
    "\n",
    "usa_airport_df = usa_airport_df.union(new_airports_df).distinct()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "36a1773748b3093d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Check that now we have coordinates for each airport\n",
    "print(distinct_airports.subtract(usa_iata_codes).isEmpty())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "35d8e478b44c087",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Now we merge the two dataframes\n",
    "flights_with_origin = df.join(usa_airport_df, df[\"Origin\"] == usa_airport_df[\"iata_code\"])\n",
    "flights_with_origin = flights_with_origin.withColumnRenamed(\"latitude_deg\", \"Origin_Lat\").withColumnRenamed(\"longitude_deg\", \"Origin_Long\")\n",
    "\n",
    "flights_with_origin.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21372a9ee122191f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#adding destination coordinates\n",
    "usa_airport_df = usa_airport_df.distinct()\n",
    "flights_with_dest = flights_with_origin.join(usa_airport_df, flights_with_origin[\"Dest\"] == usa_airport_df[\"iata_code\"])\n",
    "flights_with_dest = flights_with_dest.withColumnRenamed(\"latitude_deg\", \"Dest_Lat\").withColumnRenamed(\"longitude_deg\", \"Dest_Long\")\n",
    "\n",
    "flights_with_dest.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61c8bd6501d1a272",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Now we have the coordinates of the airports instead of the codes.\n",
    "\n",
    "merged_df = flights_with_dest.drop(\"iata_code\",\"Origin\",\"Dest\")\n",
    "\n",
    "merged_df.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "94cd71e2b645f996",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Use OneHotEncoder for UniqueCarrier"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f1f8aeca275ff42"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Now we transform the 'UniqueCarrrier' feature in a OneHotEncoder\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Create a StringIndexer\n",
    "indexer = StringIndexer(inputCol=\"UniqueCarrier\", outputCol=\"UniqueCarrierIndex\")\n",
    "\n",
    "# Fit the indexer to the DataFrame and transform it\n",
    "df_indexed = indexer.fit(merged_df).transform(merged_df)\n",
    "\n",
    "# Create a OneHotEncoder\n",
    "encoder = OneHotEncoder(inputCols=[\"UniqueCarrierIndex\"], outputCols=[\"UniqueCarrierVec\"])\n",
    "\n",
    "# Apply the encoder to the DataFrame\n",
    "df_encoded = encoder.fit(df_indexed).transform(df_indexed)\n",
    "df_encoded.select(\"UniqueCarrier\", \"UniqueCarrierVec\").distinct().show(truncate=False)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9374c337a31f37f3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Print Results\n",
    "df_encoded.select(\"UniqueCarrier\", \"UniqueCarrierVec\").distinct().show(truncate=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b227d30d242c6eef",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "final_df = df_encoded.drop(\"UniqueCarrier\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4bb7092e48babc1",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plot to check the coordinates values"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bcfafbfce8a24553"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Plot the coordinates to inspect data\n",
    "lat_long_df = final_df.select(\"Origin_Lat\", \"Origin_Long\").toPandas()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "93d6ae02d816d4bb",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# Taking a random sample of the data if it's too large\n",
    "sample_df = lat_long_df.sample(frac=1)  # Adjust frac as needed\n",
    "\n",
    "# Convert the 'Origin_Lat' and 'Origin_Long' columns to numeric (floats)\n",
    "sample_df['Origin_Lat'] = pd.to_numeric(sample_df['Origin_Lat'], errors='coerce')\n",
    "sample_df['Origin_Long'] = pd.to_numeric(sample_df['Origin_Long'], errors='coerce')\n",
    "\n",
    "# Check the conversion\n",
    "print(sample_df.dtypes)\n",
    "\n",
    "# Now create the scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x=sample_df[\"Origin_Long\"], y=sample_df[\"Origin_Lat\"], alpha=1,s=2)\n",
    "plt.title(\"Scatter Plot of Origin Coordinates\")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e74079d00f040f8c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming your DataFrame is named sample_df and has been converted to numeric types\n",
    "\n",
    "# Calculate Q1, Q3, and IQR for Latitude\n",
    "Q1_lat = sample_df['Origin_Lat'].quantile(0.25)\n",
    "Q3_lat = sample_df['Origin_Lat'].quantile(0.75)\n",
    "IQR_lat = Q3_lat - Q1_lat\n",
    "\n",
    "# Calculate Q1, Q3, and IQR for Longitude\n",
    "Q1_long = sample_df['Origin_Long'].quantile(0.25)\n",
    "Q3_long = sample_df['Origin_Long'].quantile(0.75)\n",
    "IQR_long = Q3_long - Q1_long\n",
    "\n",
    "# Define bounds for outliers\n",
    "lower_bound_lat = Q1_lat - 1.5 * IQR_lat\n",
    "upper_bound_lat = Q3_lat + 1.5 * IQR_lat\n",
    "lower_bound_long = Q1_long - 1.5 * IQR_long\n",
    "upper_bound_long = Q3_long + 1.5 * IQR_long\n",
    "\n",
    "# Filter out outliers\n",
    "filtered_df = sample_df[(sample_df['Origin_Lat'] >= lower_bound_lat) & \n",
    "                        (sample_df['Origin_Lat'] <= upper_bound_lat) &\n",
    "                        (sample_df['Origin_Long'] >= lower_bound_long) & \n",
    "                        (sample_df['Origin_Long'] <= upper_bound_long)]\n",
    "\n",
    "# Now create the scatter plot with smaller points and without outliers\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x=filtered_df[\"Origin_Long\"], y=filtered_df[\"Origin_Lat\"], alpha=0.1, s=2)\n",
    "plt.title(\"Scatter Plot of Origin Coordinates (Outliers Removed)\")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f594c2c47d65fb6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "30ca468aa2ad3434",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4dbe2739-b21b-4f0e-a506-ab6d6e834374",
   "metadata": {},
   "source": [
    "## Variable creation"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d3e6af21191fddc9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "66c07786201c399d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abee2f2-cce6-4102-ac53-295b123f6327",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b4ea91b-eed6-4c75-a259-574b91ae2856",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6044f3cc-7adc-436e-ae23-b23ec00537b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30c8d2ab-1654-4868-9e55-1af0ef9d4ae8",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97493a44-4e9f-447c-b82b-5ba63fbf39be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Close the context"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e715faa356fe098"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "spark.stop()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1bff8e312af807e9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_venv",
   "language": "python",
   "name": "pyspark_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
