{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a40d36499bbcc0a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T20:54:17.463458265Z",
     "start_time": "2024-01-07T20:54:12.932940986Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Create a SparkSession\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"App\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1f02403b8d20b1",
   "metadata": {},
   "source": [
    "## Load CSV File with Header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ddc359a0ca93d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T20:54:47.364857729Z",
     "start_time": "2024-01-07T20:54:42.509483212Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Load the CSV file with the first row as a header\n",
    "df = spark.read.format(\"csv\").option(\"delimiter\", \",\").option(\"header\", \"true\").load(\"1987.csv\")\n",
    "\n",
    "# Display the columns and the first 15 rows\n",
    "df.show(15, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0a44fd-92a8-4c32-9acd-ba2a71946d10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T20:54:52.912042106Z",
     "start_time": "2024-01-07T20:54:51.599533181Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Iterate over all columns in the DataFrame\n",
    "for column in df.columns:\n",
    "    df = df.withColumn(column, when(col(column) == \"NA\", None).otherwise(col(column)))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baba2d3d-2306-44ab-a74d-095f157c2e07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T20:55:00.535833137Z",
     "start_time": "2024-01-07T20:54:54.955126713Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Counts the number of null values for each column\n",
    "df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0e748d-08b5-4a81-bd26-5eb6b287c520",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "- eliminate unnecesary variables\n",
    "- missing and duplicates values\n",
    "- see correlation\n",
    "- variable transformation\n",
    "- variable creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19529b89-a015-4f2a-a06f-1b699d5b9cb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T20:55:03.851152743Z",
     "start_time": "2024-01-07T20:55:03.808436760Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# columns to eliminate\n",
    "columns = [\n",
    "    \"ArrTime\", \n",
    "    \"ActualElapsedTime\", \n",
    "    \"AirTime\", \n",
    "    \"TaxiIn\", \n",
    "    \"Diverted\", \n",
    "    \"CarrierDelay\", \n",
    "    \"WeatherDelay\", \n",
    "    \"NASDelay\", \n",
    "    \"SecurityDelay\", \n",
    "    \"LateAircraftDelay\"\n",
    "]\n",
    "\n",
    "# Eliminate columns\n",
    "df = df.drop(*columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07a7991-2e7e-4bbc-8b42-060f43af487c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T20:55:30.343535410Z",
     "start_time": "2024-01-07T20:55:30.279008885Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# columns to eliminate\n",
    "columns = [\n",
    "    \"Year\",\n",
    "    \"TailNum\",\n",
    "    \"TaxiOut\",\n",
    "    \"Cancelled\",\n",
    "    \"CancellationCode\"  \n",
    "]\n",
    "\n",
    "# Eliminate columns\n",
    "df = df.drop(*columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6081fe72-7891-41b4-8766-e90e8691304d",
   "metadata": {},
   "source": [
    "## Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4648b34a-0027-4839-9895-917333067aef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T20:55:43.267983632Z",
     "start_time": "2024-01-07T20:55:40.634590659Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Contar valores NA por columna\n",
    "df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66b2cce1bc21e91",
   "metadata": {},
   "source": [
    "#Now we check if NA stands for 0. If this value is not present, means that NA was 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87379505-3215-428c-8bbe-7b734213ae6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T20:55:54.132501975Z",
     "start_time": "2024-01-07T20:55:51.667432906Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "# Filter the DataFrame to keep only rows where ArrDelay is equal to 0\n",
    "filtered_df = df.filter(F.col(\"ArrDelay\") == 0)\n",
    "\n",
    "# Total number of rows in the DataFrame\n",
    "total_rows = df.count()\n",
    "\n",
    "# Check if there are any rows in the filtered DataFrame\n",
    "if filtered_df.count() > 0:\n",
    "    print(\"0 is present in the ArrDelay column \" + str(filtered_df.count()) + \" times out of \" + str(total_rows) + \".\")\n",
    "else:\n",
    "    print(\"0 is not present in the ArrDelay column.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0df10f9-d1d4-4d2a-b254-828a9caac563",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T20:55:59.838483714Z",
     "start_time": "2024-01-07T20:55:57.542926120Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the percentage of null values for each column\n",
    "null_percentage = df.select([(count(when(col(c).isNull(), c)) / total_rows).alias(c) for c in df.columns])\n",
    "\n",
    "# Show the percentage of null values for each column\n",
    "null_percentage.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5ff12d-d8ee-40b4-aa65-441f1e07c540",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T20:56:03.743208451Z",
     "start_time": "2024-01-07T20:56:00.860727451Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop rows with at least one missing value\n",
    "\n",
    "df = df.dropna()\n",
    "dropped_rows = total_rows - df.count()\n",
    "print(\"Dropped \"+ str(dropped_rows)+ \" rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee9a02d-dd7e-43fd-9e56-7c38ef749809",
   "metadata": {},
   "source": [
    "## Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53d608a-f26a-4550-b9f9-67f0dfc32658",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T20:56:21.008748131Z",
     "start_time": "2024-01-07T20:56:11.074394567Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check for duplicates and show the results\n",
    "total_rows = df.count()\n",
    "df = df.dropDuplicates()\n",
    "\n",
    "if total_rows - df.count()  > 0:\n",
    "    print(\"There are duplicates in the DataFrame.\")\n",
    "else:\n",
    "    print(\"No duplicates found in the DataFrame.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d694f480-0a4f-4bd8-888e-1571a67c45a0",
   "metadata": {},
   "source": [
    "## Variable transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096b53ac-d091-49c6-b989-07346ac53b4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T20:56:33.405810262Z",
     "start_time": "2024-01-07T20:56:28.249724091Z"
    }
   },
   "outputs": [],
   "source": [
    "# List of columns to exclude from conversion\n",
    "exclude_columns = ['UniqueCarrier', 'Origin', 'Dest']\n",
    "\n",
    "# Convert all columns to integer type except the ones in exclude_columns\n",
    "for column in df.columns:\n",
    "    if column not in exclude_columns:\n",
    "        df = df.withColumn(column, col(column).cast(\"integer\"))\n",
    "\n",
    "# Display the columns and the first 15 rows to verify the change\n",
    "df.show(15, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14deb4462c6a2749",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T20:56:45.162008627Z",
     "start_time": "2024-01-07T20:56:37.381389689Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Print distinct values for each specified column\n",
    "for column in exclude_columns:\n",
    "    print(f\"Distinct values in column '{column}':\")\n",
    "    distinct_values = df.select(column).distinct().collect()\n",
    "    for value in distinct_values:\n",
    "        print(value[column])\n",
    "    print(\"\\n\")  # Adding a newline for better readability\n",
    "    # Print the number of elements in the distinct_values list\n",
    "    print(f\"Number of distinct values: {len(distinct_values)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d883a4781dc30abe",
   "metadata": {},
   "source": [
    "## Import geographic coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e90499fbadf24a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T20:57:14.107623649Z",
     "start_time": "2024-01-07T20:57:13.891126004Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "usa_airport_df = spark.read.csv(\"us-airports.csv\", header=True)\n",
    "\n",
    "# Select only the 'latitude_deg', 'longitude_deg', and 'iata_code' columns\n",
    "usa_airport_df = usa_airport_df.select(\"latitude_deg\", \"longitude_deg\", \"iata_code\")\n",
    "\n",
    "usa_airport_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc69cee8f70ed35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T20:57:47.103758508Z",
     "start_time": "2024-01-07T20:57:38.825757652Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Select and concatenate the values from 'Origin' and 'Destination' columns\n",
    "origin = df.select(\"Origin\").distinct()\n",
    "destination = df.select(\"Dest\").distinct()\n",
    "\n",
    "all_airports = origin.distinct().union(destination)\n",
    "\n",
    "# Get distinct values\n",
    "distinct_airports = all_airports.distinct()\n",
    "print(all_airports.count())\n",
    "\n",
    "\n",
    "\n",
    "# Collect the distinct values\n",
    "\n",
    "#Distinct airport codes in the new Dataset.\n",
    "usa_iata_codes = usa_airport_df.select(\"iata_code\").distinct()\n",
    "\n",
    "\n",
    "#Show the missing values\n",
    "\n",
    "distinct_airports.subtract(usa_iata_codes).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a1773748b3093d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T20:59:31.205416673Z",
     "start_time": "2024-01-07T20:59:30.259908648Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to convert DMS (degrees, minutes, seconds) to DD (decimal degrees)\n",
    "def dms_to_dd(dms):\n",
    "    parts = re.split('[°′″]', dms)\n",
    "    degrees = float(parts[0])\n",
    "    minutes = float(parts[1])\n",
    "    seconds = float(parts[2])\n",
    "    direction = parts[3]\n",
    "    \n",
    "    dd = degrees + minutes/60 + seconds/3600\n",
    "    if direction in ('S', 'W'):\n",
    "        dd *= -1\n",
    "    return dd\n",
    "\n",
    "# Coordinates in DMS format with corresponding  codes\n",
    "coordinates = [\n",
    "    (\"43°08′36″N\", \"075°22′48″W\",\"UCA\"),\n",
    "    (\"18°20′14″N\", \"064°58′24″W\",\"STT\"),\n",
    "    (\"7°22′02″N\", \"134°32′39″E\",\"ROR\"),\n",
    "    (\"17°42′16″N\", \"64°48′06″W\",\"STX\"),\n",
    "    (\"9°29′56″N\", \"138°04′57″E\",\"YAP\"),\n",
    "    (\"13°29′02″N\", \"144°47′50″W\",\"GUM\"),\n",
    "    (\"15°07′08″N\", \"145°43′46″E\",\"SPN\"),\n",
    "    (\"30°12′44″N\", \"085°40′58″W\",\"PFN\"),\n",
    "    (\"18°26′22″N\", \"66°00′07″W\",\"SJU\")\n",
    "]\n",
    "\n",
    "# Convert the DMS coordinates to decimal degrees\n",
    "airport_coordinates_dd = [(dms_to_dd(lat), dms_to_dd(lon),code) for  lat, lon ,code in coordinates]\n",
    "\n",
    "new_airports_df = spark.createDataFrame(airport_coordinates_dd, [\"latitude_deg\", \"longitude_deg\", \"iata_code\"])\n",
    "new_airports_df.show()\n",
    "\n",
    "usa_airport_df = usa_airport_df.union(new_airports_df).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d8e478b44c087",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T21:03:51.381206770Z",
     "start_time": "2024-01-07T21:03:44.652291229Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#Check that now we have coordinates for each airport\n",
    "distinct_airports.subtract(usa_airport_df.select(\"iata_code\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21372a9ee122191f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T21:05:03.514933547Z",
     "start_time": "2024-01-07T21:04:05.368383893Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Now we merge the two dataframes\n",
    "flights_with_origin = df.join(usa_airport_df, df[\"Origin\"] == usa_airport_df[\"iata_code\"])\n",
    "flights_with_origin = flights_with_origin.withColumnRenamed(\"latitude_deg\", \"Origin_Lat\").withColumnRenamed(\"longitude_deg\", \"Origin_Long\")\n",
    "\n",
    "#flights_with_origin.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c8bd6501d1a272",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T21:07:18.395015077Z",
     "start_time": "2024-01-07T21:05:37.063801312Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#adding destination coordinates\n",
    "usa_airport_df = usa_airport_df.distinct()\n",
    "flights_with_dest = flights_with_origin.join(usa_airport_df, flights_with_origin[\"Dest\"] == usa_airport_df[\"iata_code\"])\n",
    "flights_with_dest = flights_with_dest.withColumnRenamed(\"latitude_deg\", \"Dest_Lat\").withColumnRenamed(\"longitude_deg\", \"Dest_Long\")\n",
    "\n",
    "#flights_with_dest.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cd71e2b645f996",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T21:09:07.417309213Z",
     "start_time": "2024-01-07T21:07:22.523299526Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#Now we have the coordinates of the airports instead of the codes.\n",
    "\n",
    "merged_df = flights_with_dest.drop(\"iata_code\",\"Origin\",\"Dest\")\n",
    "\n",
    "#merged_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1f8aeca275ff42",
   "metadata": {},
   "source": [
    "## Use OneHotEncoder for UniqueCarrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9374c337a31f37f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T21:13:21.135007825Z",
     "start_time": "2024-01-07T21:09:53.857290560Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#Now we transform the 'UniqueCarrrier' feature in a OneHotEncoder\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "#from pyspark.ml import Pipeline\n",
    "\n",
    "# Create a StringIndexer\n",
    "indexer = StringIndexer(inputCol=\"UniqueCarrier\", outputCol=\"UniqueCarrierIndex\")\n",
    "\n",
    "# Fit the indexer to the DataFrame and transform it\n",
    "df_indexed = indexer.fit(merged_df).transform(merged_df)\n",
    "\n",
    "# Create a OneHotEncoder\n",
    "encoder = OneHotEncoder(inputCols=[\"UniqueCarrierIndex\"], outputCols=[\"UniqueCarrierVec\"])\n",
    "\n",
    "# Apply the encoder to the DataFrame\n",
    "df_encoded = encoder.fit(df_indexed).transform(df_indexed)\n",
    "df_encoded.select(\"UniqueCarrier\", \"UniqueCarrierVec\").distinct().show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b227d30d242c6eef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T21:15:15.895618474Z",
     "start_time": "2024-01-07T21:13:32.225711216Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#Print Results\n",
    "df_encoded.select(\"UniqueCarrier\", \"UniqueCarrierVec\").distinct().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bb7092e48babc1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T21:48:41.323254762Z",
     "start_time": "2024-01-07T21:48:41.227138504Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "final_df = df_encoded.drop(\"UniqueCarrier\")\n",
    "\n",
    "\n",
    "# Convert \"Origini_Lat\" from string to double\n",
    "final_df = final_df.withColumn(\"Origin_Lat\", final_df[\"Origin_Lat\"].cast(\"double\"))\n",
    "\n",
    "# Convert \"Origini_Long\" from string to double\n",
    "final_df = final_df.withColumn(\"Origin_Long\", final_df[\"Origin_Long\"].cast(\"double\"))\n",
    "\n",
    "# Convert \"Dest_Lat\" from string to double\n",
    "final_df = final_df.withColumn(\"Dest_Lat\", final_df[\"Dest_Lat\"].cast(\"double\"))\n",
    "\n",
    "# Convert \"Dest_Long\" from string to double\n",
    "final_df = final_df.withColumn(\"Dest_Long\", final_df[\"Dest_Long\"].cast(\"double\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfafbfce8a24553",
   "metadata": {},
   "source": [
    "## Plot to check the coordinates values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d6ae02d816d4bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T21:17:38.771031788Z",
     "start_time": "2024-01-07T21:15:54.583284189Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot the coordinates to inspect data\n",
    "lat_long_df = final_df.select(\"Origin_Lat\", \"Origin_Long\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74079d00f040f8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T21:18:57.682919196Z",
     "start_time": "2024-01-07T21:18:54.440494742Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# Taking a random sample of the data if it's too large\n",
    "sample_df = lat_long_df.sample(frac=1)  # Adjust frac as needed\n",
    "\n",
    "# Convert the 'Origin_Lat' and 'Origin_Long' columns to numeric (floats)\n",
    "sample_df['Origin_Lat'] = pd.to_numeric(sample_df['Origin_Lat'], errors='coerce')\n",
    "sample_df['Origin_Long'] = pd.to_numeric(sample_df['Origin_Long'], errors='coerce')\n",
    "\n",
    "# Check the conversion\n",
    "print(sample_df.dtypes)\n",
    "\n",
    "# Now create the scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x=sample_df[\"Origin_Long\"], y=sample_df[\"Origin_Lat\"], alpha=1,s=2)\n",
    "plt.title(\"Scatter Plot of Origin Coordinates\")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf3fa85-0cac-4b8b-b3d7-c6032fd22ae8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f594c2c47d65fb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T21:19:02.447784103Z",
     "start_time": "2024-01-07T21:19:01.148436513Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming your DataFrame is named sample_df and has been converted to numeric types\n",
    "\n",
    "# Calculate Q1, Q3, and IQR for Latitude\n",
    "Q1_lat = sample_df['Origin_Lat'].quantile(0.25)\n",
    "Q3_lat = sample_df['Origin_Lat'].quantile(0.75)\n",
    "IQR_lat = Q3_lat - Q1_lat\n",
    "\n",
    "# Calculate Q1, Q3, and IQR for Longitude\n",
    "Q1_long = sample_df['Origin_Long'].quantile(0.25)\n",
    "Q3_long = sample_df['Origin_Long'].quantile(0.75)\n",
    "IQR_long = Q3_long - Q1_long\n",
    "\n",
    "# Define bounds for outliers\n",
    "lower_bound_lat = Q1_lat - 1.5 * IQR_lat\n",
    "upper_bound_lat = Q3_lat + 1.5 * IQR_lat\n",
    "lower_bound_long = Q1_long - 1.5 * IQR_long\n",
    "upper_bound_long = Q3_long + 1.5 * IQR_long\n",
    "\n",
    "# Filter out outliers\n",
    "filtered_df = sample_df[(sample_df['Origin_Lat'] >= lower_bound_lat) & \n",
    "                        (sample_df['Origin_Lat'] <= upper_bound_lat) &\n",
    "                        (sample_df['Origin_Long'] >= lower_bound_long) & \n",
    "                        (sample_df['Origin_Long'] <= upper_bound_long)]\n",
    "\n",
    "# Now create the scatter plot with smaller points and without outliers\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x=filtered_df[\"Origin_Long\"], y=filtered_df[\"Origin_Lat\"], alpha=0.1, s=2)\n",
    "plt.title(\"Scatter Plot of Origin Coordinates (Outliers Removed)\")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27e49f6911b55f2",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc6d0f2a064c470",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T21:51:36.070285098Z",
     "start_time": "2024-01-07T21:49:40.442573045Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "# Select the numerical columns you want to include in the correlation analysis\n",
    "numerical_cols = [col_name for col_name, data_type in final_df.dtypes if data_type == 'int' or data_type == 'double']\n",
    "\n",
    "\n",
    "\n",
    "# Create a vector assembler to assemble the features\n",
    "assembler = VectorAssembler(inputCols=numerical_cols, outputCol=\"features\")\n",
    "assembled_df = assembler.transform(final_df)\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "corr_matrix = Correlation.corr(assembled_df, \"features\").head()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f89c0cf59ccad1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T21:56:49.627215257Z",
     "start_time": "2024-01-07T21:56:48.612753037Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Convert the correlation matrix to a Pandas DataFrame for visualization\n",
    "corr_df = pd.DataFrame(corr_matrix.toArray(), columns=numerical_cols, index=numerical_cols)\n",
    "\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_df, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75516ce2a0aa986",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T22:10:48.915210114Z",
     "start_time": "2024-01-07T22:10:48.852092452Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#We drop some columns that are higly correlated with another\n",
    "\n",
    "final_df = final_df.drop(\"CRSElapsedTime\",\"CRSElapsedTime\",\"CRSArrTime\",\"CRSDepTime\")\n",
    "\n",
    "final_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbe2739-b21b-4f0e-a506-ab6d6e834374",
   "metadata": {},
   "source": [
    "## Variable creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564a20e6-30d0-4573-88f5-fe293f3441d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# 1. Define classes based on ArrDelay\n",
    "# Categorize flights as \"Not Late\" (0) if ArrDelay <= 0, and \"Late\" (1) otherwise\n",
    "final_df = final_df.withColumn(\"ArrDelayClass\", when(final_df[\"ArrDelay\"] <= 0, 0).otherwise(1))\n",
    "\n",
    "# Count the frequency of each class in the 'label' column\n",
    "class_counts = final_df.groupBy(\"ArrDelayClass\").count().orderBy(\"ArrDelayClass\")\n",
    "\n",
    "# Show the class counts\n",
    "class_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e6af21191fddc9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Import required functions from PySpark\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Calculate ratio\n",
    "major_df = df.filter(col(\"ArrDelayClass\") == 1)\n",
    "minor_df = df.filter(col(\"ArrDelayClass\") == 0)\n",
    "\n",
    "# Calculate the oversampling ratio\n",
    "ratio = int(major_df.count() / minor_df.count())\n",
    "print(f\"Oversampling ratio: {ratio}\")\n",
    "\n",
    "# Duplicate the minority rows based on the calculated ratio\n",
    "oversampled_df = minor_df.withColumn(\"dummy\", F.explode(F.array([F.lit(x) for x in range(ratio)]))).drop(\"dummy\")\n",
    "\n",
    "# Combine the oversampled minority rows with the major rows\n",
    "combined_df = major_df.unionAll(oversampled_df)\n",
    "\n",
    "# Count the frequency of each class in the 'ArrDelayClass' column after oversampling\n",
    "class_counts = combined_df.groupBy(\"ArrDelayClass\").count().orderBy(\"ArrDelayClass\")\n",
    "\n",
    "# Show the class counts\n",
    "class_counts.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4ea91b-eed6-4c75-a259-574b91ae2856",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6044f3cc-7adc-436e-ae23-b23ec00537b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T22:13:37.368222140Z",
     "start_time": "2024-01-07T22:13:37.307645272Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcedbb89-1e38-42ac-bc4f-6910172aae2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec67287e565ee072",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T22:13:38.796507813Z",
     "start_time": "2024-01-07T22:13:38.716357887Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Assuming your target variable is \"ArrDelay\" and your feature columns are selected\n",
    "feature_columns = [\"Month\", \"DayofMonth\", \"DayOfWeek\", \"DepTime\", \"FlightNum\", \"DepDelay\", \"UniqueCarrierIndex\"]\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "assembled_data = assembler.transform(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38121c4-645f-4f54-bd4b-79e0ed4e9f6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T22:13:38.796507813Z",
     "start_time": "2024-01-07T22:13:38.716357887Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Assuming your target variable is \"ArrDelay\" and your feature columns are selected\n",
    "feature_columns = [\"Month\", \"DayofMonth\", \"DayOfWeek\", \"DepTime\", \"FlightNum\", \"DepDelay\", \"Distance\", \"Origin_Lat\", \"Origin_Long\", \"Dest_Lat\", \"Dest_Long\", \"UniqueCarrierIndex\"]\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "assembled_data = assembler.transform(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f051b950b26e2815",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T22:13:41.222336282Z",
     "start_time": "2024-01-07T22:13:41.183763323Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_data, test_data = assembled_data.randomSplit([0.7, 0.3], seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50200798e53ee12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T22:30:10.169040353Z",
     "start_time": "2024-01-07T22:25:01.172783338Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"ArrDelay\",regParam=0.01)\n",
    "lr_model = lr.fit(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a56dc357e8c6a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T22:57:41.786503100Z",
     "start_time": "2024-01-07T22:56:01.578791996Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "test_results = lr_model.evaluate(test_data)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Root Mean Squared Error (RMSE):\", test_results.rootMeanSquaredError)\n",
    "print(\"R-squared (R2):\", test_results.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bb4640-459f-4d79-9b97-be4ad28a276e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForestClassifier\n",
    "\n",
    "# set rf model\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "# instantiate pipeline\n",
    "pipeline = Pipeline(stages=[assembled_data, rf])\n",
    "\n",
    "# train model\n",
    "model_rf = pipeline.fit(train_data)\n",
    "\n",
    "# create prediction column on test data\n",
    "results = model_rf.transform(test_data)\n",
    "\n",
    "# evaluate results\n",
    "# Define the hyperparameter grid\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [10, 20, 30]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10, 15]) \\\n",
    "    .build()\n",
    "\n",
    "# Create the cross-validator\n",
    "cross_validator = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"accuracy\"),\n",
    "                          numFolds=5, seed=42)\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "cv_model = cross_validator.fit(train_data)\n",
    "\n",
    "best_rf_model = cv_model.bestModel.stages[-1]\n",
    "importances = best_rf_model.featureImportances\n",
    "\n",
    "\n",
    "print(\"Feature Importances:\")\n",
    "for feature, importance in zip(feature_columns, importances):\n",
    "    print(f\"{feature}: {importance:.4f}\")\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = cv_model.transform(test_data)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"accuracy\")\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = {:.2f}\".format(accuracy))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e950c51-4ba4-4f3a-b9dd-571a53f32ced",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fb3143-6038-40cd-8d20-7af8ce34ea5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de74a2a2-ad42-43ab-92a8-2807497fd793",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30c8d2ab-1654-4868-9e55-1af0ef9d4ae8",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97493a44-4e9f-447c-b82b-5ba63fbf39be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e715faa356fe098",
   "metadata": {},
   "source": [
    "##Close the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bff8e312af807e9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd038da-ed53-4f7b-bb1f-8fdcb96ae722",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1818235-80df-4e64-a689-cf3894b7cc84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d9efad-c2ff-41f8-8feb-d344b70a8d09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22e4795-a976-47fa-bacb-b2e5e0d2f694",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d759b454-e9a4-4c87-be2a-4a4816635893",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96e5dce-956b-436a-b739-3cf45ce5f726",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9625e2a-5d86-4b09-91b9-63cc01d28f7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f428f2-9e0a-4b11-bbf2-9277d109e38f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_venv",
   "language": "python",
   "name": "pyspark_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
