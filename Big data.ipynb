{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a40d36499bbcc0a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T20:54:17.463458265Z",
     "start_time": "2024-01-07T20:54:12.932940986Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"App\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1f02403b8d20b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load CSV File with Header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29ddc359a0ca93d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T20:54:47.364857729Z",
     "start_time": "2024-01-07T20:54:42.509483212Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|Year|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|ArrTime|CRSArrTime|UniqueCarrier|FlightNum|TailNum|ActualElapsedTime|CRSElapsedTime|AirTime|ArrDelay|DepDelay|Origin|Dest|Distance|TaxiIn|TaxiOut|Cancelled|CancellationCode|Diverted|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|1987|10   |14        |3        |741    |730       |912    |849       |PS           |1451     |NA     |91               |79            |NA     |23      |11      |SAN   |SFO |447     |NA    |NA     |0        |NA              |0       |NA          |NA          |NA      |NA           |NA               |\n",
      "|1987|10   |15        |4        |729    |730       |903    |849       |PS           |1451     |NA     |94               |79            |NA     |14      |-1      |SAN   |SFO |447     |NA    |NA     |0        |NA              |0       |NA          |NA          |NA      |NA           |NA               |\n",
      "|1987|10   |17        |6        |741    |730       |918    |849       |PS           |1451     |NA     |97               |79            |NA     |29      |11      |SAN   |SFO |447     |NA    |NA     |0        |NA              |0       |NA          |NA          |NA      |NA           |NA               |\n",
      "|1987|10   |18        |7        |729    |730       |847    |849       |PS           |1451     |NA     |78               |79            |NA     |-2      |-1      |SAN   |SFO |447     |NA    |NA     |0        |NA              |0       |NA          |NA          |NA      |NA           |NA               |\n",
      "|1987|10   |19        |1        |749    |730       |922    |849       |PS           |1451     |NA     |93               |79            |NA     |33      |19      |SAN   |SFO |447     |NA    |NA     |0        |NA              |0       |NA          |NA          |NA      |NA           |NA               |\n",
      "|1987|10   |21        |3        |728    |730       |848    |849       |PS           |1451     |NA     |80               |79            |NA     |-1      |-2      |SAN   |SFO |447     |NA    |NA     |0        |NA              |0       |NA          |NA          |NA      |NA           |NA               |\n",
      "|1987|10   |22        |4        |728    |730       |852    |849       |PS           |1451     |NA     |84               |79            |NA     |3       |-2      |SAN   |SFO |447     |NA    |NA     |0        |NA              |0       |NA          |NA          |NA      |NA           |NA               |\n",
      "|1987|10   |23        |5        |731    |730       |902    |849       |PS           |1451     |NA     |91               |79            |NA     |13      |1       |SAN   |SFO |447     |NA    |NA     |0        |NA              |0       |NA          |NA          |NA      |NA           |NA               |\n",
      "|1987|10   |24        |6        |744    |730       |908    |849       |PS           |1451     |NA     |84               |79            |NA     |19      |14      |SAN   |SFO |447     |NA    |NA     |0        |NA              |0       |NA          |NA          |NA      |NA           |NA               |\n",
      "|1987|10   |25        |7        |729    |730       |851    |849       |PS           |1451     |NA     |82               |79            |NA     |2       |-1      |SAN   |SFO |447     |NA    |NA     |0        |NA              |0       |NA          |NA          |NA      |NA           |NA               |\n",
      "|1987|10   |26        |1        |735    |730       |904    |849       |PS           |1451     |NA     |89               |79            |NA     |15      |5       |SAN   |SFO |447     |NA    |NA     |0        |NA              |0       |NA          |NA          |NA      |NA           |NA               |\n",
      "|1987|10   |28        |3        |741    |725       |919    |855       |PS           |1451     |NA     |98               |90            |NA     |24      |16      |SAN   |SFO |447     |NA    |NA     |0        |NA              |0       |NA          |NA          |NA      |NA           |NA               |\n",
      "|1987|10   |29        |4        |742    |725       |906    |855       |PS           |1451     |NA     |84               |90            |NA     |11      |17      |SAN   |SFO |447     |NA    |NA     |0        |NA              |0       |NA          |NA          |NA      |NA           |NA               |\n",
      "|1987|10   |31        |6        |726    |725       |848    |855       |PS           |1451     |NA     |82               |90            |NA     |-7      |1       |SAN   |SFO |447     |NA    |NA     |0        |NA              |0       |NA          |NA          |NA      |NA           |NA               |\n",
      "|1987|10   |1         |4        |936    |915       |1035   |1001      |PS           |1451     |NA     |59               |46            |NA     |34      |21      |SFO   |RNO |192     |NA    |NA     |0        |NA              |0       |NA          |NA          |NA      |NA           |NA               |\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file with the first row as a header\n",
    "df = spark.read.format(\"csv\").option(\"delimiter\", \",\").option(\"header\", \"true\").load(\"1987.csv\")\n",
    "\n",
    "# Display the columns and the first 15 rows\n",
    "df.show(15, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e0a44fd-92a8-4c32-9acd-ba2a71946d10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T20:54:52.912042106Z",
     "start_time": "2024-01-07T20:54:51.599533181Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|Year|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|ArrTime|CRSArrTime|UniqueCarrier|FlightNum|TailNum|ActualElapsedTime|CRSElapsedTime|AirTime|ArrDelay|DepDelay|Origin|Dest|Distance|TaxiIn|TaxiOut|Cancelled|CancellationCode|Diverted|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|1987|   10|        14|        3|    741|       730|    912|       849|           PS|     1451|   NULL|               91|            79|   NULL|      23|      11|   SAN| SFO|     447|  NULL|   NULL|        0|            NULL|       0|        NULL|        NULL|    NULL|         NULL|             NULL|\n",
      "|1987|   10|        15|        4|    729|       730|    903|       849|           PS|     1451|   NULL|               94|            79|   NULL|      14|      -1|   SAN| SFO|     447|  NULL|   NULL|        0|            NULL|       0|        NULL|        NULL|    NULL|         NULL|             NULL|\n",
      "|1987|   10|        17|        6|    741|       730|    918|       849|           PS|     1451|   NULL|               97|            79|   NULL|      29|      11|   SAN| SFO|     447|  NULL|   NULL|        0|            NULL|       0|        NULL|        NULL|    NULL|         NULL|             NULL|\n",
      "|1987|   10|        18|        7|    729|       730|    847|       849|           PS|     1451|   NULL|               78|            79|   NULL|      -2|      -1|   SAN| SFO|     447|  NULL|   NULL|        0|            NULL|       0|        NULL|        NULL|    NULL|         NULL|             NULL|\n",
      "|1987|   10|        19|        1|    749|       730|    922|       849|           PS|     1451|   NULL|               93|            79|   NULL|      33|      19|   SAN| SFO|     447|  NULL|   NULL|        0|            NULL|       0|        NULL|        NULL|    NULL|         NULL|             NULL|\n",
      "|1987|   10|        21|        3|    728|       730|    848|       849|           PS|     1451|   NULL|               80|            79|   NULL|      -1|      -2|   SAN| SFO|     447|  NULL|   NULL|        0|            NULL|       0|        NULL|        NULL|    NULL|         NULL|             NULL|\n",
      "|1987|   10|        22|        4|    728|       730|    852|       849|           PS|     1451|   NULL|               84|            79|   NULL|       3|      -2|   SAN| SFO|     447|  NULL|   NULL|        0|            NULL|       0|        NULL|        NULL|    NULL|         NULL|             NULL|\n",
      "|1987|   10|        23|        5|    731|       730|    902|       849|           PS|     1451|   NULL|               91|            79|   NULL|      13|       1|   SAN| SFO|     447|  NULL|   NULL|        0|            NULL|       0|        NULL|        NULL|    NULL|         NULL|             NULL|\n",
      "|1987|   10|        24|        6|    744|       730|    908|       849|           PS|     1451|   NULL|               84|            79|   NULL|      19|      14|   SAN| SFO|     447|  NULL|   NULL|        0|            NULL|       0|        NULL|        NULL|    NULL|         NULL|             NULL|\n",
      "|1987|   10|        25|        7|    729|       730|    851|       849|           PS|     1451|   NULL|               82|            79|   NULL|       2|      -1|   SAN| SFO|     447|  NULL|   NULL|        0|            NULL|       0|        NULL|        NULL|    NULL|         NULL|             NULL|\n",
      "|1987|   10|        26|        1|    735|       730|    904|       849|           PS|     1451|   NULL|               89|            79|   NULL|      15|       5|   SAN| SFO|     447|  NULL|   NULL|        0|            NULL|       0|        NULL|        NULL|    NULL|         NULL|             NULL|\n",
      "|1987|   10|        28|        3|    741|       725|    919|       855|           PS|     1451|   NULL|               98|            90|   NULL|      24|      16|   SAN| SFO|     447|  NULL|   NULL|        0|            NULL|       0|        NULL|        NULL|    NULL|         NULL|             NULL|\n",
      "|1987|   10|        29|        4|    742|       725|    906|       855|           PS|     1451|   NULL|               84|            90|   NULL|      11|      17|   SAN| SFO|     447|  NULL|   NULL|        0|            NULL|       0|        NULL|        NULL|    NULL|         NULL|             NULL|\n",
      "|1987|   10|        31|        6|    726|       725|    848|       855|           PS|     1451|   NULL|               82|            90|   NULL|      -7|       1|   SAN| SFO|     447|  NULL|   NULL|        0|            NULL|       0|        NULL|        NULL|    NULL|         NULL|             NULL|\n",
      "|1987|   10|         1|        4|    936|       915|   1035|      1001|           PS|     1451|   NULL|               59|            46|   NULL|      34|      21|   SFO| RNO|     192|  NULL|   NULL|        0|            NULL|       0|        NULL|        NULL|    NULL|         NULL|             NULL|\n",
      "|1987|   10|         2|        5|    918|       915|   1017|      1001|           PS|     1451|   NULL|               59|            46|   NULL|      16|       3|   SFO| RNO|     192|  NULL|   NULL|        0|            NULL|       0|        NULL|        NULL|    NULL|         NULL|             NULL|\n",
      "|1987|   10|         3|        6|    928|       915|   1037|      1001|           PS|     1451|   NULL|               69|            46|   NULL|      36|      13|   SFO| RNO|     192|  NULL|   NULL|        0|            NULL|       0|        NULL|        NULL|    NULL|         NULL|             NULL|\n",
      "|1987|   10|         4|        7|    914|       915|   1003|      1001|           PS|     1451|   NULL|               49|            46|   NULL|       2|      -1|   SFO| RNO|     192|  NULL|   NULL|        0|            NULL|       0|        NULL|        NULL|    NULL|         NULL|             NULL|\n",
      "|1987|   10|         5|        1|   1042|       915|   1129|      1001|           PS|     1451|   NULL|               47|            46|   NULL|      88|      87|   SFO| RNO|     192|  NULL|   NULL|        0|            NULL|       0|        NULL|        NULL|    NULL|         NULL|             NULL|\n",
      "|1987|   10|         6|        2|    934|       915|   1024|      1001|           PS|     1451|   NULL|               50|            46|   NULL|      23|      19|   SFO| RNO|     192|  NULL|   NULL|        0|            NULL|       0|        NULL|        NULL|    NULL|         NULL|             NULL|\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Iterate over all columns in the DataFrame\n",
    "for column in df.columns:\n",
    "    df = df.withColumn(column, when(col(column) == \"NA\", None).otherwise(col(column)))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "baba2d3d-2306-44ab-a74d-095f157c2e07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T20:55:00.535833137Z",
     "start_time": "2024-01-07T20:54:54.955126713Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+-------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|Year|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|ArrTime|CRSArrTime|UniqueCarrier|FlightNum|TailNum|ActualElapsedTime|CRSElapsedTime|AirTime|ArrDelay|DepDelay|Origin|Dest|Distance| TaxiIn|TaxiOut|Cancelled|CancellationCode|Diverted|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+-------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|   0|    0|         0|        0|  19685|         0|  23500|         0|            0|        0|1311826|            23500|             0|1311826|   23500|   19685|     0|   0|    1015|1311826|1311826|        0|         1311826|       0|     1311826|     1311826| 1311826|      1311826|          1311826|\n",
      "+----+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-------+-----------------+--------------+-------+--------+--------+------+----+--------+-------+-------+---------+----------------+--------+------------+------------+--------+-------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Counts the number of null values for each column\n",
    "df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0e748d-08b5-4a81-bd26-5eb6b287c520",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "- eliminate unnecesary variables\n",
    "- missing and duplicates values\n",
    "- see correlation\n",
    "- variable transformation\n",
    "- variable creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19529b89-a015-4f2a-a06f-1b699d5b9cb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T20:55:03.851152743Z",
     "start_time": "2024-01-07T20:55:03.808436760Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# columns to eliminate\n",
    "columns = [\n",
    "    \"ArrTime\", \n",
    "    \"ActualElapsedTime\", \n",
    "    \"AirTime\", \n",
    "    \"TaxiIn\", \n",
    "    \"Diverted\", \n",
    "    \"CarrierDelay\", \n",
    "    \"WeatherDelay\", \n",
    "    \"NASDelay\", \n",
    "    \"SecurityDelay\", \n",
    "    \"LateAircraftDelay\"\n",
    "]\n",
    "\n",
    "# Eliminate columns\n",
    "df = df.drop(*columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f07a7991-2e7e-4bbc-8b42-060f43af487c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T20:55:30.343535410Z",
     "start_time": "2024-01-07T20:55:30.279008885Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# columns to eliminate\n",
    "columns = [\n",
    "    \"Year\",\n",
    "    \"TailNum\",\n",
    "    \"TaxiOut\",\n",
    "    \"Cancelled\",\n",
    "    \"CancellationCode\"  \n",
    "]\n",
    "\n",
    "# Eliminate columns\n",
    "df = df.drop(*columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6081fe72-7891-41b4-8766-e90e8691304d",
   "metadata": {},
   "source": [
    "## Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4648b34a-0027-4839-9895-917333067aef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T20:55:43.267983632Z",
     "start_time": "2024-01-07T20:55:40.634590659Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-----------------+--------------+-------+--------+--------+------+----+--------+-------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|ArrTime|CRSArrTime|UniqueCarrier|FlightNum|ActualElapsedTime|CRSElapsedTime|AirTime|ArrDelay|DepDelay|Origin|Dest|Distance| TaxiIn|Diverted|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|\n",
      "+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-----------------+--------------+-------+--------+--------+------+----+--------+-------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|    0|         0|        0|  19685|         0|  23500|         0|            0|        0|            23500|             0|1311826|   23500|   19685|     0|   0|    1015|1311826|       0|     1311826|     1311826| 1311826|      1311826|          1311826|\n",
      "+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-----------------+--------------+-------+--------+--------+------+----+--------+-------+--------+------------+------------+--------+-------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Contar valores NA por columna\n",
    "df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66b2cce1bc21e91",
   "metadata": {},
   "source": [
    "#Now we check if NA stands for 0. If this value is not present, means that NA was 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87379505-3215-428c-8bbe-7b734213ae6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T20:55:54.132501975Z",
     "start_time": "2024-01-07T20:55:51.667432906Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 is present in the ArrDelay column 60436 times out of 1311826.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "# Filter the DataFrame to keep only rows where ArrDelay is equal to 0\n",
    "filtered_df = df.filter(F.col(\"ArrDelay\") == 0)\n",
    "\n",
    "# Total number of rows in the DataFrame\n",
    "total_rows = df.count()\n",
    "\n",
    "# Check if there are any rows in the filtered DataFrame\n",
    "if filtered_df.count() > 0:\n",
    "    print(\"0 is present in the ArrDelay column \" + str(filtered_df.count()) + \" times out of \" + str(total_rows) + \".\")\n",
    "else:\n",
    "    print(\"0 is not present in the ArrDelay column.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0df10f9-d1d4-4d2a-b254-828a9caac563",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T20:55:59.838483714Z",
     "start_time": "2024-01-07T20:55:57.542926120Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+---------+--------------------+----------+--------------------+----------+-------------+---------+--------------------+--------------+-------+--------------------+--------------------+------+----+--------------------+------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|Month|DayofMonth|DayOfWeek|             DepTime|CRSDepTime|             ArrTime|CRSArrTime|UniqueCarrier|FlightNum|   ActualElapsedTime|CRSElapsedTime|AirTime|            ArrDelay|            DepDelay|Origin|Dest|            Distance|TaxiIn|Diverted|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|\n",
      "+-----+----------+---------+--------------------+----------+--------------------+----------+-------------+---------+--------------------+--------------+-------+--------------------+--------------------+------+----+--------------------+------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|  0.0|       0.0|      0.0|0.015005801074227831|       0.0|0.017913961150335486|       0.0|          0.0|      0.0|0.017913961150335486|           0.0|    1.0|0.017913961150335486|0.015005801074227831|   0.0| 0.0|7.737306624506603E-4|   1.0|     0.0|         1.0|         1.0|     1.0|          1.0|              1.0|\n",
      "+-----+----------+---------+--------------------+----------+--------------------+----------+-------------+---------+--------------------+--------------+-------+--------------------+--------------------+------+----+--------------------+------+--------+------------+------------+--------+-------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the percentage of null values for each column\n",
    "null_percentage = df.select([(count(when(col(c).isNull(), c)) / total_rows).alias(c) for c in df.columns])\n",
    "\n",
    "# Show the percentage of null values for each column\n",
    "null_percentage.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a5ff12d-d8ee-40b4-aa65-441f1e07c540",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T20:56:03.743208451Z",
     "start_time": "2024-01-07T20:56:00.860727451Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 1311826 rows.\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with at least one missing value\n",
    "\n",
    "df = df.dropna()\n",
    "dropped_rows = total_rows - df.count()\n",
    "print(\"Dropped \"+ str(dropped_rows)+ \" rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee9a02d-dd7e-43fd-9e56-7c38ef749809",
   "metadata": {},
   "source": [
    "## Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a53d608a-f26a-4550-b9f9-67f0dfc32658",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T20:56:21.008748131Z",
     "start_time": "2024-01-07T20:56:11.074394567Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicates found in the DataFrame.\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates and show the results\n",
    "total_rows = df.count()\n",
    "df = df.dropDuplicates()\n",
    "\n",
    "if total_rows - df.count()  > 0:\n",
    "    print(\"There are duplicates in the DataFrame.\")\n",
    "else:\n",
    "    print(\"No duplicates found in the DataFrame.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d694f480-0a4f-4bd8-888e-1571a67c45a0",
   "metadata": {},
   "source": [
    "## Variable transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "096b53ac-d091-49c6-b989-07346ac53b4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T20:56:33.405810262Z",
     "start_time": "2024-01-07T20:56:28.249724091Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-----------------+--------------+-------+--------+--------+------+----+--------+------+--------+------------+------------+--------+-------------+-----------------+\n",
      "|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|ArrTime|CRSArrTime|UniqueCarrier|FlightNum|ActualElapsedTime|CRSElapsedTime|AirTime|ArrDelay|DepDelay|Origin|Dest|Distance|TaxiIn|Diverted|CarrierDelay|WeatherDelay|NASDelay|SecurityDelay|LateAircraftDelay|\n",
      "+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-----------------+--------------+-------+--------+--------+------+----+--------+------+--------+------------+------------+--------+-------------+-----------------+\n",
      "+-----+----------+---------+-------+----------+-------+----------+-------------+---------+-----------------+--------------+-------+--------+--------+------+----+--------+------+--------+------------+------------+--------+-------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List of columns to exclude from conversion\n",
    "exclude_columns = ['UniqueCarrier', 'Origin', 'Dest']\n",
    "\n",
    "# Convert all columns to integer type except the ones in exclude_columns\n",
    "for column in df.columns:\n",
    "    if column not in exclude_columns:\n",
    "        df = df.withColumn(column, col(column).cast(\"integer\"))\n",
    "\n",
    "# Display the columns and the first 15 rows to verify the change\n",
    "df.show(15, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14deb4462c6a2749",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T20:56:45.162008627Z",
     "start_time": "2024-01-07T20:56:37.381389689Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct values in column 'UniqueCarrier':\n",
      "\n",
      "\n",
      "Number of distinct values: 0\n",
      "Distinct values in column 'Origin':\n",
      "\n",
      "\n",
      "Number of distinct values: 0\n",
      "Distinct values in column 'Dest':\n",
      "\n",
      "\n",
      "Number of distinct values: 0\n"
     ]
    }
   ],
   "source": [
    "# Print distinct values for each specified column\n",
    "for column in exclude_columns:\n",
    "    print(f\"Distinct values in column '{column}':\")\n",
    "    distinct_values = df.select(column).distinct().collect()\n",
    "    for value in distinct_values:\n",
    "        print(value[column])\n",
    "    print(\"\\n\")  # Adding a newline for better readability\n",
    "    # Print the number of elements in the distinct_values list\n",
    "    print(f\"Number of distinct values: {len(distinct_values)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d883a4781dc30abe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Import geographic coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14e90499fbadf24a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T20:57:14.107623649Z",
     "start_time": "2024-01-07T20:57:13.891126004Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+--------------------+\n",
      "|      latitude_deg|     longitude_deg|           iata_code|\n",
      "+------------------+------------------+--------------------+\n",
      "|         #geo +lat|         #geo +lon|#loc +airport +co...|\n",
      "|         33.942501|       -118.407997|                 LAX|\n",
      "|           41.9786|          -87.9048|                 ORD|\n",
      "|         40.639447|        -73.779317|                 JFK|\n",
      "|           33.6367|        -84.428101|                 ATL|\n",
      "| 37.61899948120117|          -122.375|                 SFO|\n",
      "|         40.692501|        -74.168701|                 EWR|\n",
      "|         32.896801|        -97.038002|                 DFW|\n",
      "|         36.083361|       -115.151817|                 LAS|\n",
      "|28.429399490356445|-81.30899810791016|                 MCO|\n",
      "|   39.861698150635|    -104.672996521|                 DEN|\n",
      "|           38.9445|        -77.455803|                 IAD|\n",
      "|         40.777199|        -73.872597|                 LGA|\n",
      "| 25.79319953918457|-80.29060363769531|                 MIA|\n",
      "|         33.435302|       -112.005905|                 PHX|\n",
      "|         47.449162|       -122.311134|                 SEA|\n",
      "|           42.3643|        -71.005203|                 BOS|\n",
      "| 39.87189865112305|-75.24109649658203|                 PHL|\n",
      "|  35.2140007019043|-80.94309997558594|                 CLT|\n",
      "|29.984399795532227|-95.34140014648438|                 IAH|\n",
      "+------------------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file\n",
    "usa_airport_df = spark.read.csv(\"us-airports.csv\", header=True)\n",
    "\n",
    "# Select only the 'latitude_deg', 'longitude_deg', and 'iata_code' columns\n",
    "usa_airport_df = usa_airport_df.select(\"latitude_deg\", \"longitude_deg\", \"iata_code\")\n",
    "\n",
    "usa_airport_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8fc69cee8f70ed35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T20:57:47.103758508Z",
     "start_time": "2024-01-07T20:57:38.825757652Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472\n",
      "+------+\n",
      "|Origin|\n",
      "+------+\n",
      "|   SPN|\n",
      "|   UCA|\n",
      "|   SJU|\n",
      "|   YAP|\n",
      "|   STX|\n",
      "|   PFN|\n",
      "|   GUM|\n",
      "|   ROR|\n",
      "|   STT|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select and concatenate the values from 'Origin' and 'Destination' columns\n",
    "origin = df.select(\"Origin\").distinct()\n",
    "destination = df.select(\"Dest\").distinct()\n",
    "\n",
    "all_airports = origin.distinct().union(destination)\n",
    "\n",
    "# Get distinct values\n",
    "distinct_airports = all_airports.distinct()\n",
    "print(all_airports.count())\n",
    "\n",
    "\n",
    "\n",
    "# Collect the distinct values\n",
    "\n",
    "#Distinct airport codes in the new Dataset.\n",
    "usa_iata_codes = usa_airport_df.select(\"iata_code\").distinct()\n",
    "\n",
    "\n",
    "#Show the missing values\n",
    "\n",
    "distinct_airports.subtract(usa_iata_codes).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36a1773748b3093d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T20:59:31.205416673Z",
     "start_time": "2024-01-07T20:59:30.259908648Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o630.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 68.0 failed 1 times, most recent failure: Lost task 0.0 in stage 68.0 (TID 172) (Claudia.technicolor.net executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.base/java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readInt(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\t... 26 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3326)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3549)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readInt(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\t... 26 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m airport_coordinates_dd \u001b[38;5;241m=\u001b[39m [(dms_to_dd(lat), dms_to_dd(lon),code) \u001b[38;5;28;01mfor\u001b[39;00m  lat, lon ,code \u001b[38;5;129;01min\u001b[39;00m coordinates]\n\u001b[0;32m     32\u001b[0m new_airports_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(airport_coordinates_dd, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatitude_deg\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlongitude_deg\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miata_code\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m \u001b[43mnew_airports_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m usa_airport_df \u001b[38;5;241m=\u001b[39m usa_airport_df\u001b[38;5;241m.\u001b[39munion(new_airports_df)\u001b[38;5;241m.\u001b[39mdistinct()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:959\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    953\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    954\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    955\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    956\u001b[0m     )\n\u001b[0;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 959\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    960\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    961\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o630.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 68.0 failed 1 times, most recent failure: Lost task 0.0 in stage 68.0 (TID 172) (Claudia.technicolor.net executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.base/java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readInt(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\t... 26 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3326)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3549)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readInt(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\t... 26 more\r\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Function to convert DMS (degrees, minutes, seconds) to DD (decimal degrees)\n",
    "def dms_to_dd(dms):\n",
    "    parts = re.split('[°′″]', dms)\n",
    "    degrees = float(parts[0])\n",
    "    minutes = float(parts[1])\n",
    "    seconds = float(parts[2])\n",
    "    direction = parts[3]\n",
    "    \n",
    "    dd = degrees + minutes/60 + seconds/3600\n",
    "    if direction in ('S', 'W'):\n",
    "        dd *= -1\n",
    "    return dd\n",
    "\n",
    "# Coordinates in DMS format with corresponding  codes\n",
    "coordinates = [\n",
    "    (\"43°08′36″N\", \"075°22′48″W\",\"UCA\"),\n",
    "    (\"18°20′14″N\", \"064°58′24″W\",\"STT\"),\n",
    "    (\"7°22′02″N\", \"134°32′39″E\",\"ROR\"),\n",
    "    (\"17°42′16″N\", \"64°48′06″W\",\"STX\"),\n",
    "    (\"9°29′56″N\", \"138°04′57″E\",\"YAP\"),\n",
    "    (\"13°29′02″N\", \"144°47′50″W\",\"GUM\"),\n",
    "    (\"15°07′08″N\", \"145°43′46″E\",\"SPN\"),\n",
    "    (\"30°12′44″N\", \"085°40′58″W\",\"PFN\"),\n",
    "    (\"18°26′22″N\", \"66°00′07″W\",\"SJU\")\n",
    "]\n",
    "\n",
    "# Convert the DMS coordinates to decimal degrees\n",
    "airport_coordinates_dd = [(dms_to_dd(lat), dms_to_dd(lon),code) for  lat, lon ,code in coordinates]\n",
    "\n",
    "new_airports_df = spark.createDataFrame(airport_coordinates_dd, [\"latitude_deg\", \"longitude_deg\", \"iata_code\"])\n",
    "new_airports_df.show()\n",
    "\n",
    "usa_airport_df = usa_airport_df.union(new_airports_df).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d8e478b44c087",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T21:03:51.381206770Z",
     "start_time": "2024-01-07T21:03:44.652291229Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#Check that now we have coordinates for each airport\n",
    "distinct_airports.subtract(usa_airport_df.select(\"iata_code\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21372a9ee122191f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T21:05:03.514933547Z",
     "start_time": "2024-01-07T21:04:05.368383893Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Now we merge the two dataframes\n",
    "flights_with_origin = df.join(usa_airport_df, df[\"Origin\"] == usa_airport_df[\"iata_code\"])\n",
    "flights_with_origin = flights_with_origin.withColumnRenamed(\"latitude_deg\", \"Origin_Lat\").withColumnRenamed(\"longitude_deg\", \"Origin_Long\")\n",
    "\n",
    "#flights_with_origin.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c8bd6501d1a272",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T21:07:18.395015077Z",
     "start_time": "2024-01-07T21:05:37.063801312Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#adding destination coordinates\n",
    "usa_airport_df = usa_airport_df.distinct()\n",
    "flights_with_dest = flights_with_origin.join(usa_airport_df, flights_with_origin[\"Dest\"] == usa_airport_df[\"iata_code\"])\n",
    "flights_with_dest = flights_with_dest.withColumnRenamed(\"latitude_deg\", \"Dest_Lat\").withColumnRenamed(\"longitude_deg\", \"Dest_Long\")\n",
    "\n",
    "#flights_with_dest.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cd71e2b645f996",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T21:09:07.417309213Z",
     "start_time": "2024-01-07T21:07:22.523299526Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#Now we have the coordinates of the airports instead of the codes.\n",
    "\n",
    "merged_df = flights_with_dest.drop(\"iata_code\",\"Origin\",\"Dest\")\n",
    "\n",
    "#merged_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1f8aeca275ff42",
   "metadata": {},
   "source": [
    "## Use OneHotEncoder for UniqueCarrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9374c337a31f37f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T21:13:21.135007825Z",
     "start_time": "2024-01-07T21:09:53.857290560Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'merged_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m indexer \u001b[38;5;241m=\u001b[39m StringIndexer(inputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUniqueCarrier\u001b[39m\u001b[38;5;124m\"\u001b[39m, outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUniqueCarrierIndex\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Fit the indexer to the DataFrame and transform it\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m df_indexed \u001b[38;5;241m=\u001b[39m indexer\u001b[38;5;241m.\u001b[39mfit(\u001b[43mmerged_df\u001b[49m)\u001b[38;5;241m.\u001b[39mtransform(merged_df)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Create a OneHotEncoder\u001b[39;00m\n\u001b[0;32m     13\u001b[0m encoder \u001b[38;5;241m=\u001b[39m OneHotEncoder(inputCols\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUniqueCarrierIndex\u001b[39m\u001b[38;5;124m\"\u001b[39m], outputCols\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUniqueCarrierVec\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'merged_df' is not defined"
     ]
    }
   ],
   "source": [
    "#Now we transform the 'UniqueCarrrier' feature in a OneHotEncoder\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "#from pyspark.ml import Pipeline\n",
    "\n",
    "# Create a StringIndexer\n",
    "indexer = StringIndexer(inputCol=\"UniqueCarrier\", outputCol=\"UniqueCarrierIndex\")\n",
    "\n",
    "# Fit the indexer to the DataFrame and transform it\n",
    "df_indexed = indexer.fit(merged_df).transform(merged_df)\n",
    "\n",
    "# Create a OneHotEncoder\n",
    "encoder = OneHotEncoder(inputCols=[\"UniqueCarrierIndex\"], outputCols=[\"UniqueCarrierVec\"])\n",
    "\n",
    "# Apply the encoder to the DataFrame\n",
    "df_encoded = encoder.fit(df_indexed).transform(df_indexed)\n",
    "df_encoded.select(\"UniqueCarrier\", \"UniqueCarrierVec\").distinct().show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b227d30d242c6eef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T21:15:15.895618474Z",
     "start_time": "2024-01-07T21:13:32.225711216Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#Print Results\n",
    "df_encoded.select(\"UniqueCarrier\", \"UniqueCarrierVec\").distinct().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bb7092e48babc1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T21:48:41.323254762Z",
     "start_time": "2024-01-07T21:48:41.227138504Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "final_df = df_encoded.drop(\"UniqueCarrier\")\n",
    "\n",
    "\n",
    "# Convert \"Origini_Lat\" from string to double\n",
    "final_df = final_df.withColumn(\"Origin_Lat\", final_df[\"Origin_Lat\"].cast(\"double\"))\n",
    "\n",
    "# Convert \"Origini_Long\" from string to double\n",
    "final_df = final_df.withColumn(\"Origin_Long\", final_df[\"Origin_Long\"].cast(\"double\"))\n",
    "\n",
    "# Convert \"Dest_Lat\" from string to double\n",
    "final_df = final_df.withColumn(\"Dest_Lat\", final_df[\"Dest_Lat\"].cast(\"double\"))\n",
    "\n",
    "# Convert \"Dest_Long\" from string to double\n",
    "final_df = final_df.withColumn(\"Dest_Long\", final_df[\"Dest_Long\"].cast(\"double\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfafbfce8a24553",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Plot to check the coordinates values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d6ae02d816d4bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T21:17:38.771031788Z",
     "start_time": "2024-01-07T21:15:54.583284189Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot the coordinates to inspect data\n",
    "lat_long_df = final_df.select(\"Origin_Lat\", \"Origin_Long\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74079d00f040f8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T21:18:57.682919196Z",
     "start_time": "2024-01-07T21:18:54.440494742Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# Taking a random sample of the data if it's too large\n",
    "sample_df = lat_long_df.sample(frac=1)  # Adjust frac as needed\n",
    "\n",
    "# Convert the 'Origin_Lat' and 'Origin_Long' columns to numeric (floats)\n",
    "sample_df['Origin_Lat'] = pd.to_numeric(sample_df['Origin_Lat'], errors='coerce')\n",
    "sample_df['Origin_Long'] = pd.to_numeric(sample_df['Origin_Long'], errors='coerce')\n",
    "\n",
    "# Check the conversion\n",
    "print(sample_df.dtypes)\n",
    "\n",
    "# Now create the scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x=sample_df[\"Origin_Long\"], y=sample_df[\"Origin_Lat\"], alpha=1,s=2)\n",
    "plt.title(\"Scatter Plot of Origin Coordinates\")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf3fa85-0cac-4b8b-b3d7-c6032fd22ae8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f594c2c47d65fb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T21:19:02.447784103Z",
     "start_time": "2024-01-07T21:19:01.148436513Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming your DataFrame is named sample_df and has been converted to numeric types\n",
    "\n",
    "# Calculate Q1, Q3, and IQR for Latitude\n",
    "Q1_lat = sample_df['Origin_Lat'].quantile(0.25)\n",
    "Q3_lat = sample_df['Origin_Lat'].quantile(0.75)\n",
    "IQR_lat = Q3_lat - Q1_lat\n",
    "\n",
    "# Calculate Q1, Q3, and IQR for Longitude\n",
    "Q1_long = sample_df['Origin_Long'].quantile(0.25)\n",
    "Q3_long = sample_df['Origin_Long'].quantile(0.75)\n",
    "IQR_long = Q3_long - Q1_long\n",
    "\n",
    "# Define bounds for outliers\n",
    "lower_bound_lat = Q1_lat - 1.5 * IQR_lat\n",
    "upper_bound_lat = Q3_lat + 1.5 * IQR_lat\n",
    "lower_bound_long = Q1_long - 1.5 * IQR_long\n",
    "upper_bound_long = Q3_long + 1.5 * IQR_long\n",
    "\n",
    "# Filter out outliers\n",
    "filtered_df = sample_df[(sample_df['Origin_Lat'] >= lower_bound_lat) & \n",
    "                        (sample_df['Origin_Lat'] <= upper_bound_lat) &\n",
    "                        (sample_df['Origin_Long'] >= lower_bound_long) & \n",
    "                        (sample_df['Origin_Long'] <= upper_bound_long)]\n",
    "\n",
    "# Now create the scatter plot with smaller points and without outliers\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x=filtered_df[\"Origin_Long\"], y=filtered_df[\"Origin_Lat\"], alpha=0.1, s=2)\n",
    "plt.title(\"Scatter Plot of Origin Coordinates (Outliers Removed)\")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27e49f6911b55f2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc6d0f2a064c470",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T21:51:36.070285098Z",
     "start_time": "2024-01-07T21:49:40.442573045Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "# Select the numerical columns you want to include in the correlation analysis\n",
    "numerical_cols = [col_name for col_name, data_type in final_df.dtypes if data_type == 'int' or data_type == 'double']\n",
    "\n",
    "\n",
    "\n",
    "# Create a vector assembler to assemble the features\n",
    "assembler = VectorAssembler(inputCols=numerical_cols, outputCol=\"features\")\n",
    "assembled_df = assembler.transform(final_df)\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "corr_matrix = Correlation.corr(assembled_df, \"features\").head()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f89c0cf59ccad1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T21:56:49.627215257Z",
     "start_time": "2024-01-07T21:56:48.612753037Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Convert the correlation matrix to a Pandas DataFrame for visualization\n",
    "corr_df = pd.DataFrame(corr_matrix.toArray(), columns=numerical_cols, index=numerical_cols)\n",
    "\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_df, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75516ce2a0aa986",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T22:10:48.915210114Z",
     "start_time": "2024-01-07T22:10:48.852092452Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#We drop some columns that are higly correlated with another\n",
    "\n",
    "final_df = final_df.drop(\"CRSElapsedTime\",\"CRSElapsedTime\",\"CRSArrTime\",\"CRSDepTime\")\n",
    "\n",
    "final_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbe2739-b21b-4f0e-a506-ab6d6e834374",
   "metadata": {},
   "source": [
    "## Variable creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "564a20e6-30d0-4573-88f5-fe293f3441d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|ArrDelayClass|count|\n",
      "+-------------+-----+\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# 1. Define classes based on ArrDelay\n",
    "# Categorize flights as \"Not Late\" (0) if ArrDelay <= 0, and \"Late\" (1) otherwise\n",
    "final_df = final_df.withColumn(\"ArrDelayClass\", when(final_df[\"ArrDelay\"] <= 0, 0).otherwise(1))\n",
    "\n",
    "# Count the frequency of each class in the 'label' column\n",
    "class_counts = final_df.groupBy(\"ArrDelayClass\").count().orderBy(\"ArrDelayClass\")\n",
    "\n",
    "# Show the class counts\n",
    "class_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "364a4e68-17eb-4e3e-81d2-4a538bb7050b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m a_count \u001b[38;5;241m=\u001b[39m df_a\u001b[38;5;241m.\u001b[39mcount()\n\u001b[0;32m      5\u001b[0m b_count \u001b[38;5;241m=\u001b[39m df_b\u001b[38;5;241m.\u001b[39mcount() \n\u001b[1;32m----> 6\u001b[0m ratio \u001b[38;5;241m=\u001b[39m \u001b[43ma_count\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mb_count\u001b[49m\n\u001b[0;32m      8\u001b[0m df_b_oversampled \u001b[38;5;241m=\u001b[39m df_b\u001b[38;5;241m.\u001b[39msample(withReplacement\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, fraction\u001b[38;5;241m=\u001b[39mratio, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      9\u001b[0m combined_df \u001b[38;5;241m=\u001b[39m df_a\u001b[38;5;241m.\u001b[39munionAll(df_b_oversampled)\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "df_a = final_df.filter(df['ArrDelayClass'] == 1)\n",
    "df_b = final_df.filter(df['ArrDelayClass'] == 0)\n",
    "\n",
    "a_count = df_a.count()\n",
    "b_count = df_b.count() \n",
    "ratio = a_count / b_count\n",
    "\n",
    "df_b_oversampled = df_b.sample(withReplacement=True, fraction=ratio, seed=1)\n",
    "combined_df = df_a.unionAll(df_b_oversampled)\n",
    "\n",
    "# Count the frequency of each class in the 'ArrDelayClass' column after oversampling\n",
    "class_counts = combined_df.groupBy(\"ArrDelayClass\").count().orderBy(\"ArrDelayClass\")\n",
    "\n",
    "# Show the class counts\n",
    "class_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d168eb6b-5d3a-4b38-b054-960a92df3163",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+---------+-------+----------+----------+-------------+---------+--------------+--------+--------+------+----+--------+-------------+\n",
      "|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|CRSArrTime|UniqueCarrier|FlightNum|CRSElapsedTime|ArrDelay|DepDelay|Origin|Dest|Distance|ArrDelayClass|\n",
      "+-----+----------+---------+-------+----------+----------+-------------+---------+--------------+--------+--------+------+----+--------+-------------+\n",
      "|   10|        10|        6|   1635|      1635|      1732|           PS|     1454|            57|       8|       0|   OAK| BUR|     325|            1|\n",
      "|   10|        20|        2|   1945|      1945|      2101|           PS|     1484|            76|       1|       0|   SFO| SAN|     447|            1|\n",
      "|   10|         9|        5|   2057|      2016|      2132|           PS|     1486|            76|      47|      41|   OAK| SAN|     446|            1|\n",
      "|   10|        13|        2|   2114|      2115|      2223|           PS|     1505|            68|       3|      -1|   SNA| SJC|     342|            1|\n",
      "|   10|        18|        7|    841|       842|      1030|           PS|     1517|           108|       3|      -1|   SFO| GEG|     733|            1|\n",
      "|   10|        19|        1|    914|       842|      1030|           PS|     1517|           108|      52|      32|   SFO| GEG|     733|            1|\n",
      "|   10|        27|        2|   1129|      1130|      1234|           PS|     1524|            64|      22|      -1|   SFO| BUR|     326|            1|\n",
      "|   10|         4|        7|   1806|      1800|      1911|           PS|     1526|            71|       6|       6|   SFO| LAX|     337|            1|\n",
      "|   10|        27|        2|   1804|      1800|      1911|           PS|     1526|            71|      21|       4|   SFO| LAX|     337|            1|\n",
      "|   10|        11|        7|   1832|      1830|      1931|           PS|     1538|            61|       8|       2|   SFO| BUR|     326|            1|\n",
      "|   10|        21|        3|   2007|      2010|      2126|           PS|     1547|            76|      11|      -3|   TUS| LAX|     451|            1|\n",
      "|   10|        16|        5|   1008|      1010|      1117|           PS|     1551|            67|      27|      -2|   BUR| SMF|     358|            1|\n",
      "|   10|        23|        5|   1811|      1810|      1858|           PS|     1566|            48|      10|       1|   BUR| LAS|     223|            1|\n",
      "|   10|        31|        6|   1705|      1635|      1800|           PS|     1577|            85|      19|      30|   SAN| SFO|     447|            1|\n",
      "|   10|        18|        7|   2034|      2035|      2157|           PS|     1589|            82|       3|      -1|   SAN| SMF|     480|            1|\n",
      "|   10|        28|        3|   1235|      1230|      1349|           PS|     1592|            79|      12|       5|   PDX| SMF|     479|            1|\n",
      "|   10|         8|        4|   1042|      1005|      1126|           PS|     1593|            81|      39|      37|   SMF| PDX|     479|            1|\n",
      "|   10|        27|        2|   2145|      2120|      2216|           PS|     1663|            56|      29|      25|   BUR| SJC|     296|            1|\n",
      "|   10|        15|        4|   1646|      1645|      1801|           PS|     1678|            76|       5|       1|   SFO| SAN|     447|            1|\n",
      "|   10|         2|        5|   1702|      1700|      1818|           PS|     1687|            78|       7|       2|   SNA| SFO|     372|            1|\n",
      "+-----+----------+---------+-------+----------+----------+-------------+---------+--------------+--------+--------+------+----+--------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combined_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4ea91b-eed6-4c75-a259-574b91ae2856",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6044f3cc-7adc-436e-ae23-b23ec00537b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T22:13:37.368222140Z",
     "start_time": "2024-01-07T22:13:37.307645272Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'distutils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregression\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearRegression\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VectorAssembler\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtuning\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParamGridBuilder, CrossValidator\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\ml\\__init__.py:31\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     23\u001b[0m     Estimator,\n\u001b[0;32m     24\u001b[0m     Model,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     28\u001b[0m     UnaryTransformer,\n\u001b[0;32m     29\u001b[0m )\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pipeline, PipelineModel\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     32\u001b[0m     classification,\n\u001b[0;32m     33\u001b[0m     clustering,\n\u001b[0;32m     34\u001b[0m     evaluation,\n\u001b[0;32m     35\u001b[0m     feature,\n\u001b[0;32m     36\u001b[0m     fpm,\n\u001b[0;32m     37\u001b[0m     image,\n\u001b[0;32m     38\u001b[0m     recommendation,\n\u001b[0;32m     39\u001b[0m     regression,\n\u001b[0;32m     40\u001b[0m     stat,\n\u001b[0;32m     41\u001b[0m     tuning,\n\u001b[0;32m     42\u001b[0m     util,\n\u001b[0;32m     43\u001b[0m     linalg,\n\u001b[0;32m     44\u001b[0m     param,\n\u001b[0;32m     45\u001b[0m )\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TorchDistributor\n\u001b[0;32m     48\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransformer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnaryTransformer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorchDistributor\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     71\u001b[0m ]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\ml\\image.py:31\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, List, NoReturn, Optional, cast\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdistutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LooseVersion\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkContext\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Row, StructType, _create_row, _parse_datatype_json_string\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'distutils'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcedbb89-1e38-42ac-bc4f-6910172aae2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ec67287e565ee072",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T22:13:38.796507813Z",
     "start_time": "2024-01-07T22:13:38.716357887Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "UniqueCarrierIndex does not exist. Available: Month, DayofMonth, DayOfWeek, DepTime, CRSDepTime, CRSArrTime, UniqueCarrier, FlightNum, CRSElapsedTime, ArrDelay, DepDelay, Origin, Dest, Distance, ArrDelayClass",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m feature_columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMonth\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDayofMonth\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDayOfWeek\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDepTime\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFlightNum\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDepDelay\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUniqueCarrierIndex\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      3\u001b[0m assembler \u001b[38;5;241m=\u001b[39m VectorAssembler(inputCols\u001b[38;5;241m=\u001b[39mfeature_columns, outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m assembled_data \u001b[38;5;241m=\u001b[39m \u001b[43massembler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\ml\\base.py:262\u001b[0m, in \u001b[0;36mTransformer.transform\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_transform(dataset)\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be a param map but got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\ml\\wrapper.py:398\u001b[0m, in \u001b[0;36mJavaTransformer._transform\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m--> 398\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m, dataset\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m: UniqueCarrierIndex does not exist. Available: Month, DayofMonth, DayOfWeek, DepTime, CRSDepTime, CRSArrTime, UniqueCarrier, FlightNum, CRSElapsedTime, ArrDelay, DepDelay, Origin, Dest, Distance, ArrDelayClass"
     ]
    }
   ],
   "source": [
    "# Assuming your target variable is \"ArrDelay\" and your feature columns are selected\n",
    "feature_columns = [\"Month\", \"DayofMonth\", \"DayOfWeek\", \"DepTime\", \"FlightNum\", \"DepDelay\", \"UniqueCarrierIndex\"]\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "assembled_data = assembler.transform(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f051b950b26e2815",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T22:13:41.222336282Z",
     "start_time": "2024-01-07T22:13:41.183763323Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_data, test_data = assembled_data.randomSplit([0.7, 0.3], seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50200798e53ee12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T22:30:10.169040353Z",
     "start_time": "2024-01-07T22:25:01.172783338Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"ArrDelay\",regParam=0.01)\n",
    "lr_model = lr.fit(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a56dc357e8c6a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T22:57:41.786503100Z",
     "start_time": "2024-01-07T22:56:01.578791996Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "test_results = lr_model.evaluate(test_data)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Root Mean Squared Error (RMSE):\", test_results.rootMeanSquaredError)\n",
    "print(\"R-squared (R2):\", test_results.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7bb4640-459f-4d79-9b97-be4ad28a276e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'distutils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassification\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#from pyspark.ml import Pipeline\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtuning\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParamGridBuilder, CrossValidator\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mMulticlassClassificationEvaluator\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# RandomForestClassifier\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Assuming your target variable is \"ArrDelay\" and your feature columns are selected.drop([]\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\ml\\__init__.py:31\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     23\u001b[0m     Estimator,\n\u001b[0;32m     24\u001b[0m     Model,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     28\u001b[0m     UnaryTransformer,\n\u001b[0;32m     29\u001b[0m )\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pipeline, PipelineModel\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     32\u001b[0m     classification,\n\u001b[0;32m     33\u001b[0m     clustering,\n\u001b[0;32m     34\u001b[0m     evaluation,\n\u001b[0;32m     35\u001b[0m     feature,\n\u001b[0;32m     36\u001b[0m     fpm,\n\u001b[0;32m     37\u001b[0m     image,\n\u001b[0;32m     38\u001b[0m     recommendation,\n\u001b[0;32m     39\u001b[0m     regression,\n\u001b[0;32m     40\u001b[0m     stat,\n\u001b[0;32m     41\u001b[0m     tuning,\n\u001b[0;32m     42\u001b[0m     util,\n\u001b[0;32m     43\u001b[0m     linalg,\n\u001b[0;32m     44\u001b[0m     param,\n\u001b[0;32m     45\u001b[0m )\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TorchDistributor\n\u001b[0;32m     48\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransformer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnaryTransformer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorchDistributor\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     71\u001b[0m ]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\ml\\image.py:31\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, List, NoReturn, Optional, cast\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdistutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LooseVersion\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkContext\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Row, StructType, _create_row, _parse_datatype_json_string\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'distutils'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "#from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "import pyspark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "\n",
    "# RandomForestClassifier\n",
    "# Assuming your target variable is \"ArrDelay\" and your feature columns are selected.drop([]\n",
    "\n",
    "columns = [\n",
    "\"ArrDelay\"\n",
    "]\n",
    "\n",
    "# Eliminate columns\n",
    "combined_df = combined_df.drop(*columns)\n",
    "\n",
    "feature_columns = [\"Month\", \"DayofMonth\", \"DayOfWeek\", \"DepTime\", \"FlightNum\", \"DepDelay\", \"UniqueCarrierIndex\"]\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "assembled_data = assembler.transform(combined_df)\n",
    "train_data, test_data = assembled_data.randomSplit([0.7, 0.3], seed=123)\n",
    "# set rf model \n",
    "rf = RandomForestClassifier(labelCol=\"ArrDelayClass\", featuresCol=\"features\")\n",
    "\n",
    "# instantiate pipeline\n",
    "#pipeline = Pipeline(stages=[assembled_data, rf])\n",
    "\n",
    "# train model\n",
    "model_rf = rf.fit(train_data)\n",
    "\n",
    "# create prediction column on test data\n",
    "results = model_rf.transform(test_data)\n",
    "\n",
    "# evaluate results\n",
    "# Define the hyperparameter grid\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [10, 20, 30]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10, 15]) \\\n",
    "    .build()\n",
    "\n",
    "# Create the cross-validator\n",
    "cross_validator = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(labelCol=\"ArrDelayClass\", metricName=\"accuracy\"),\n",
    "                          numFolds=5, seed=42)\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "cv_model = cross_validator.fit(train_data)\n",
    "\n",
    "best_rf_model = cv_model.bestModel.stages[-1]\n",
    "importances = best_rf_model.featureImportances\n",
    "\n",
    "\n",
    "print(\"Feature Importances:\")\n",
    "for feature, importance in zip(feature_columns, importances):\n",
    "    print(f\"{feature}: {importance:.4f}\")\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = cv_model.transform(test_data)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"ArrDelayClass\", metricName=\"accuracy\")\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = {:.2f}\".format(accuracy))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e950c51-4ba4-4f3a-b9dd-571a53f32ced",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fb3143-6038-40cd-8d20-7af8ce34ea5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de74a2a2-ad42-43ab-92a8-2807497fd793",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30c8d2ab-1654-4868-9e55-1af0ef9d4ae8",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97493a44-4e9f-447c-b82b-5ba63fbf39be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e715faa356fe098",
   "metadata": {},
   "source": [
    "##Close the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bff8e312af807e9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd038da-ed53-4f7b-bb1f-8fdcb96ae722",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1818235-80df-4e64-a689-cf3894b7cc84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d9efad-c2ff-41f8-8feb-d344b70a8d09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22e4795-a976-47fa-bacb-b2e5e0d2f694",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d759b454-e9a4-4c87-be2a-4a4816635893",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96e5dce-956b-436a-b739-3cf45ce5f726",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9625e2a-5d86-4b09-91b9-63cc01d28f7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f428f2-9e0a-4b11-bbf2-9277d109e38f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_venv",
   "language": "python",
   "name": "pyspark_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
